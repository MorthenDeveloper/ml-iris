{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c827dcc-eed7-41c9-bc8f-f69f97b5a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73d9620-3af6-4137-b20b-2f3338562f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.33.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.3)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
      "Requirement already satisfied: kfp==2.2.0 in ./.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (0.15)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (1.34.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (0.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (2.0.1)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (26.1.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (3.20.3)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.2.0) (1.26.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.2.0) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.2.0) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.2.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.2.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.2.0) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.2.0) (1.16.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.2.0) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.2.0) (2.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.2.0) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.2.0) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.2.0) (68.1.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.2.0) (1.6.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.2.0) (1.3.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3,>=2.2.1->kfp==2.2.0) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.2.0) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.2.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.2.0) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.2.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform --upgrade\n",
    "!pip3 install {USER_FLAG} kfp==2.2.0 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c91b75-48c5-431a-aaf2-820acb335e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#from kfp.v2 import compiler, dsl\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20303680-1496-4088-80d0-1a4f40f319e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  fabian260497\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703abd46-4c84-4bb7-a2df-6b06e1cfcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI=\"gs://bucket-fabian-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d024331-e737-4efb-9b8e-51f813bab21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://bucket-fabian-2/vertexai_pipeline_prueba/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/vertexai_pipeline_prueba/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c86613-9ca6-4af9-ba4a-558e816dbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Component in the pipeline to fetch data from big query.\n",
    "@component\n",
    "def get_data_bigquery(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "\n",
    "    bqclient = bigquery.Client()\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_table\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    dataframe = rows.to_dataframe(\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28622173-b2ec-419e-a899-538d31451c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def extract_csv_from_gcs(\n",
    "    bucket_name: str,\n",
    "    file_path: str,\n",
    "    secret_sa: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # Cargar las credenciales del secreto\n",
    "    with open(secret_sa, \"r\") as secret_file:\n",
    "        secret_data = json.load(secret_file)\n",
    "\n",
    "    # Reemplaza 'mi-bucket' con el nombre de tu bucket y 'archivo/datos.csv' con la ruta del archivo en el bucket\n",
    "    bucket_name = bucket_name #\"bucket-fabian-2\"\n",
    "    blob_name = file_path #'data_prueba/iris_dataset_2.csv'\n",
    "\n",
    "    #credenciales_json = 'ruta-a-tu-archivo-de-credenciales.json'\n",
    "    #storage_client = storage.Client.from_service_account_json(credenciales_json)\n",
    "\n",
    "    #storage_client = storage.Client()\n",
    "    storage_client = storage.Client.from_service_account_info(secret_data)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Lee los datos del Blob en un objeto bytes\n",
    "    datos_bytes = blob.download_as_bytes()\n",
    "\n",
    "    # Convierte los datos bytes en un DataFrame de pandas\n",
    "    data_frame = pd.read_csv(io.BytesIO(datos_bytes))\n",
    "    data_frame.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8dffe12-a3d4-481a-a0bf-c2f8979cc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second component in the pipeline to train the classification model using decision Trees or Randomforest\n",
    "@component\n",
    "def training_classmod(\n",
    "    data: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Cargar el conjunto de datos Iris\n",
    "    iris_df=pd.read_csv(data1.path)\n",
    "    \n",
    "    # 2. Preprocesamiento de datos\n",
    "    X = iris_df.drop(\"target\", axis=1)\n",
    "    y = iris_df[\"target\"]\n",
    "    \n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    \n",
    "    #Entrenar el modelo\n",
    "    model_classifier = DecisionTreeClassifier()\n",
    "    #model_classifier = RandomForestClassifier()\n",
    "    model_classifier.fit(X_train,y_train)\n",
    "    \n",
    "    #Evaluar el modelo\n",
    "    y_pred=model_classifier.predict(X_test)\n",
    "    accuracy = model_classifier.score(X_test,y_test)\n",
    "    print('accuracy is:',accuracy)\n",
    "    \n",
    "    metrics.log_metric(\"accuracy\",(accuracy * 100.0))\n",
    "    metrics.log_metric(\"model\", \"Decision tree\")\n",
    "    #metrics.log_metric(\"model\", \"RandomForest\")\n",
    "    \n",
    "    #Guardamos el modelo entrenado\n",
    "    dump(model_classifier, model.path + \".joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165f4c47-3215-4fae-a646-41620bcd1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def model_deployment(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"custom-model-pipeline\", #nombre del modelo en Vertex AI\n",
    "        artifact_uri = model.uri.replace(\"model\", \"\"),  #URI del artefacto del modelo\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\" #especifica la imagen del contenedor de servicio que se utilizará para el despliegue del modelo. En este caso, se utiliza una imagen de contenedor de predicción de scikit-learn.\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-1\") #despliegue de modelo. El modelo cargado se despliega como un punto final (endpoint) que puede utilizarse para realizar inferencias (predicciones).\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = endpoint.resource_name #las URI de recursos del endpoint\n",
    "    vertex_model.uri = deployed_model.resource_name #modelo desplegado en Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0529b4-1fe5-457c-97e6-5b6d43adc741",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def generate_predictions(\n",
    "    input_data: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    output_predictions: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from joblib import load  # Para cargar el modelo\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Cargar el modelo desde Vertex AI\n",
    "    deployed_model = load(model.path + \".joblib\")\n",
    "\n",
    "    # Realizar las predicciones en tus datos de entrada (input_data)\n",
    "    input_df = pd.read_csv(input_data.path)\n",
    "    df=input_df.head(50) # usamos la misma data pero solo los 50 primeros datos\n",
    "    \n",
    "    predictions_df=df.drop(\"target\", axis=1) #obtenemos solo los features\n",
    "    \n",
    "    predictions_y = deployed_model.predict(predictions_df) #hacemos las predicciones\n",
    "    \n",
    "    #concatenamos las predicciones a las otras columnas para crear un dataframe y subirlo a bigquery\n",
    "    predictions_df['predicciones'] = predictions_y\n",
    "    \n",
    "    predictions_df.to_csv(output_predictions.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "564c3b62-8bba-4dc7-bd36-5258a9b7ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def upload_to_bigquery(\n",
    "    input_data: Input[Dataset],  # Input de tipo Dataset\n",
    "    secret_file_path:str,\n",
    "    bq_project: str,            # Proyecto de BigQuery\n",
    "    bq_dataset: str,            # Conjunto de datos de BigQuery\n",
    "    bq_table: str,              # Nombre de la tabla de BigQuery\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import json\n",
    "    \n",
    "    # Cargar las credenciales del secreto\n",
    "    with open(secret_file_path, \"r\") as secret_file:\n",
    "        secret_data = json.load(secret_file)\n",
    "    \n",
    "    # Inicializa el cliente de BigQuery\n",
    "    #bq_client = bigquery.Client()\n",
    "    bq_client = bigquery.Client.from_service_account_info(secret_data)\n",
    "\n",
    "    # Obtén la ruta del conjunto de datos y la tabla de BigQuery\n",
    "    dataset_uri = input_data.uri\n",
    "    table_uri = f\"bq://{bq_project}.{bq_dataset}.{bq_table}\"\n",
    "\n",
    "    # Lee el esquema de la tabla de BigQuery desde un archivo JSON o una cadena\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"sepal_length\", bigquery.enums.SqlTypeNames.NUMERIC, mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"sepal_width\", bigquery.enums.SqlTypeNames.NUMERIC, mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"petal_length\", bigquery.enums.SqlTypeNames.NUMERIC, mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"petal_width\", bigquery.enums.SqlTypeNames.NUMERIC, mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"target\", bigquery.enums.SqlTypeNames.NUMERIC, mode=\"NULLABLE\"),\n",
    "    ]\n",
    "    try:\n",
    "        schema = json.loads(bq_schema)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"El esquema debe estar en formato JSON válido.\")\n",
    "\n",
    "    # Carga los datos en la tabla de BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    load_job = bq_client.load_table_from_uri(dataset_uri, table_uri, job_config=job_config)\n",
    "\n",
    "    # Espera a que se complete la carga\n",
    "    load_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3458350d-e5a7-49b5-86f6-293a246237fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract-csv-from-gcs() missing 1 required argument: secret_sa.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129;43m@pipeline\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Default pipeline root. You can override it when submitting the pipeline.\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPELINE_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# A name for the pipeline.\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom-pipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mpipeline\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#bq_table: str = \"\",\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_data_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbq_project\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Proyecto de BigQuery\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbq_dataset\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Conjunto de datos de BigQuery\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbq_table\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m#Tabla de BQ\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecret_file_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mREGION\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#dataset_task = get_data_bigquery(bq_table=bq_table)\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#step 1 = Leer los datos desde el bucket de GCS.\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_task\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextract_csv_from_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_sa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecret_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#No es necesario realizar limpieza alguna\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#step 2 = Divide los datos en conjuntos de entrenamiento y prueba, entrena un modelo de machine learning\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/pipeline_context.py:65\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(func, name, description, pipeline_root, display_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[1;32m     63\u001b[0m     func\u001b[38;5;241m.\u001b[39mpipeline_root \u001b[38;5;241m=\u001b[39m pipeline_root\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomponent_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_graph_component_from_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/component_factory.py:656\u001b[0m, in \u001b[0;36mcreate_graph_component_from_func\u001b[0;34m(func, name, description, display_name)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    651\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m extract_component_interface(\n\u001b[1;32m    652\u001b[0m     func,\n\u001b[1;32m    653\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    654\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    655\u001b[0m )\n\u001b[0;32m--> 656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/graph_component.py:58\u001b[0m, in \u001b[0;36mGraphComponent.__init__\u001b[0;34m(self, component_spec, pipeline_func, display_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     50\u001b[0m         pipeline_channel\u001b[38;5;241m.\u001b[39mcreate_pipeline_channel(\n\u001b[1;32m     51\u001b[0m             name\u001b[38;5;241m=\u001b[39marg_name,\n\u001b[1;32m     52\u001b[0m             channel_type\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     53\u001b[0m             is_artifact_list\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mis_artifact_list,\n\u001b[1;32m     54\u001b[0m         ))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pipeline_context\u001b[38;5;241m.\u001b[39mPipeline(\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m---> 58\u001b[0m     pipeline_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is missing from pipeline.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 32\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(bucket_name, file_path, output_data_path, bq_project, bq_dataset, bq_table, secret_file_path, project, region)\u001b[0m\n\u001b[1;32m     29\u001b[0m deploy_task \u001b[38;5;241m=\u001b[39m model_deployment(model\u001b[38;5;241m=\u001b[39mmodel_task\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],project\u001b[38;5;241m=\u001b[39mproject,region\u001b[38;5;241m=\u001b[39mregion)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#step 4 = usaremos el modelo para hacer predicciones\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m dataset_task_2 \u001b[38;5;241m=\u001b[39m \u001b[43mextract_csv_from_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#datos para las predicciones\u001b[39;00m\n\u001b[1;32m     33\u001b[0m predict_task \u001b[38;5;241m=\u001b[39m generate_predictions(input_data\u001b[38;5;241m=\u001b[39mdataset_task_2\u001b[38;5;241m.\u001b[39moutput,model\u001b[38;5;241m=\u001b[39mdeploy_task\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_model\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#step 5 = Almacenar las predicciones en bigquery\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/dsl/base_component.py:96\u001b[0m, in \u001b[0;36mBaseComponent.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     argument_or_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m     92\u001b[0m         missing_arguments) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     94\u001b[0m         arg_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m missing_arguments)\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() missing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_arguments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m required \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_or_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline_task\u001b[38;5;241m.\u001b[39mPipelineTask(\n\u001b[1;32m    101\u001b[0m     component_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec,\n\u001b[1;32m    102\u001b[0m     args\u001b[38;5;241m=\u001b[39mtask_inputs,\n\u001b[1;32m    103\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: extract-csv-from-gcs() missing 1 required argument: secret_sa."
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline.\n",
    "    name=\"custom-pipeline\"\n",
    ")\n",
    "def pipeline(\n",
    "    #bq_table: str = \"\",\n",
    "    bucket_name: str = \"\",\n",
    "    file_path: str = \"\",\n",
    "    output_data_path: str = \"data.csv\",\n",
    "    bq_project: str = \"\",           # Proyecto de BigQuery\n",
    "    bq_dataset: str = \"\",            # Conjunto de datos de BigQuery\n",
    "    bq_table: str = \"\",              #Tabla de BQ\n",
    "    secret_file_path: str = \"\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):  \n",
    "    #dataset_task = get_data_bigquery(bq_table=bq_table)\n",
    "    #step 1 = Leer los datos desde el bucket de GCS.\n",
    "    dataset_task = extract_csv_from_gcs(bucket_name=bucket_name, file_path=file_path, secret_sa=secret_file_path)\n",
    "    \n",
    "    #No es necesario realizar limpieza alguna\n",
    "    \n",
    "    #step 2 = Divide los datos en conjuntos de entrenamiento y prueba, entrena un modelo de machine learning\n",
    "    model_task = training_classmod(data=dataset_task.output)\n",
    "    \n",
    "    #step 3 = Despliega el modelo\n",
    "    deploy_task = model_deployment(model=model_task.outputs[\"model\"],project=project,region=region)\n",
    "    \n",
    "    #step 4 = usaremos el modelo para hacer predicciones\n",
    "    dataset_task_2 = extract_csv_from_gcs(bucket_name= bucket_name,file_path= file_path) #datos para las predicciones\n",
    "    predict_task = generate_predictions(input_data=dataset_task_2.output,model=deploy_task.outputs[\"vertex_model\"])\n",
    "    \n",
    "    #step 5 = Almacenar las predicciones en bigquery\n",
    "    load_task=upload_to_bigquery(input_data=predict_task.output,secret_file_path=secret_file_path,bq_project=bq_project,bq_dataset=bq_dataset,bq_table=bq_table)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28351142-159d-412f-a42e-616fc101f426",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported pipeline_func type. Expected subclass of `base_component.BaseComponent` or `Callable` constructed with @dsl.pipeline decorator. Got: <class 'function'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Creamos la instancia del compilador de Vertex AI Pipelines\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom-pipeline-classifier.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#->ruta donde se guardará el resultado de la compilación\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/kfp/compiler/compiler.py:69\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, pipeline_func, package_path, pipeline_name, pipeline_parameters, type_check)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m type_utils\u001b[38;5;241m.\u001b[39mTypeCheckManager(enable\u001b[38;5;241m=\u001b[39mtype_check):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pipeline_func, base_component\u001b[38;5;241m.\u001b[39mBaseComponent):\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported pipeline_func type. Expected \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubclass of `base_component.BaseComponent` or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Callable` constructed with @dsl.pipeline \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecorator. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pipeline_func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     75\u001b[0m     pipeline_spec \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mmodify_pipeline_spec_with_override(\n\u001b[1;32m     76\u001b[0m         pipeline_spec\u001b[38;5;241m=\u001b[39mpipeline_func\u001b[38;5;241m.\u001b[39mpipeline_spec,\n\u001b[1;32m     77\u001b[0m         pipeline_name\u001b[38;5;241m=\u001b[39mpipeline_name,\n\u001b[1;32m     78\u001b[0m         pipeline_parameters\u001b[38;5;241m=\u001b[39mpipeline_parameters,\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     builder\u001b[38;5;241m.\u001b[39mwrite_pipeline_spec_to_file(\n\u001b[1;32m     82\u001b[0m         pipeline_spec\u001b[38;5;241m=\u001b[39mpipeline_spec,\n\u001b[1;32m     83\u001b[0m         pipeline_description\u001b[38;5;241m=\u001b[39mpipeline_func\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m     84\u001b[0m         platform_spec\u001b[38;5;241m=\u001b[39mpipeline_func\u001b[38;5;241m.\u001b[39mplatform_spec,\n\u001b[1;32m     85\u001b[0m         package_path\u001b[38;5;241m=\u001b[39mpackage_path,\n\u001b[1;32m     86\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported pipeline_func type. Expected subclass of `base_component.BaseComponent` or `Callable` constructed with @dsl.pipeline decorator. Got: <class 'function'>"
     ]
    }
   ],
   "source": [
    "#Creamos la instancia del compilador de Vertex AI Pipelines\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"custom-pipeline-classifier.json\") #->ruta donde se guardará el resultado de la compilación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7869dc-37bc-4053-b6d1-ca7027d9e7c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The pipeline parameter secret_file_path is not found in the pipeline job input definitions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run1 \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipelineJob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#->creamos una instancia de un job de Pipeline\u001b[39;49;00m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom-training-vertex-ai-pipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Nombre del pipeline\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemplate_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom-pipeline-classifier.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#la ruta del archivo json que se generó utilizando el compilador, contiene la definición del pipeline compilado\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom-pipeline-7\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#identificador único para el job del pipeline\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#parameter_values={\"bq_table\": \"fabian260497.dataset_prueba.iris\"},\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbucket_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbucket-fabian-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_prueba/iris_dataset_2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbq_project\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfabian260497\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbq_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_prueba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbq_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miris\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msecret_file_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43metc/secrets/secret-sa.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_caching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:256\u001b[0m, in \u001b[0;36mPipelineJob.__init__\u001b[0;34m(self, display_name, template_path, job_id, pipeline_root, parameter_values, input_artifacts, enable_caching, encryption_spec_key_name, labels, credentials, project, location, failure_policy)\u001b[0m\n\u001b[1;32m    253\u001b[0m builder\u001b[38;5;241m.\u001b[39mupdate_input_artifacts(input_artifacts)\n\u001b[1;32m    255\u001b[0m builder\u001b[38;5;241m.\u001b[39mupdate_failure_policy(failure_policy)\n\u001b[0;32m--> 256\u001b[0m runtime_config_dict \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m runtime_config \u001b[38;5;241m=\u001b[39m gca_pipeline_job\u001b[38;5;241m.\u001b[39mPipelineJob\u001b[38;5;241m.\u001b[39mRuntimeConfig()\u001b[38;5;241m.\u001b[39m_pb\n\u001b[1;32m    259\u001b[0m json_format\u001b[38;5;241m.\u001b[39mParseDict(runtime_config_dict, runtime_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/utils/pipeline_utils.py:186\u001b[0m, in \u001b[0;36mPipelineRuntimeConfigBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     parameter_values_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m runtime_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcsOutputDirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_root,\n\u001b[0;32m--> 186\u001b[0m     parameter_values_key: {\n\u001b[1;32m    187\u001b[0m         k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_vertex_value(k, v)\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_values\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     },\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputArtifacts\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    192\u001b[0m         k: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifactId\u001b[39m\u001b[38;5;124m\"\u001b[39m: v} \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_artifacts\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    193\u001b[0m     },\n\u001b[1;32m    194\u001b[0m }\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failure_policy:\n\u001b[1;32m    197\u001b[0m     runtime_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailurePolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failure_policy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/utils/pipeline_utils.py:187\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     parameter_values_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m runtime_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcsOutputDirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_root,\n\u001b[1;32m    186\u001b[0m     parameter_values_key: {\n\u001b[0;32m--> 187\u001b[0m         k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_vertex_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_values\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     },\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputArtifacts\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    192\u001b[0m         k: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifactId\u001b[39m\u001b[38;5;124m\"\u001b[39m: v} \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_artifacts\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    193\u001b[0m     },\n\u001b[1;32m    194\u001b[0m }\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failure_policy:\n\u001b[1;32m    197\u001b[0m     runtime_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailurePolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failure_policy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/utils/pipeline_utils.py:223\u001b[0m, in \u001b[0;36mPipelineRuntimeConfigBuilder._get_vertex_value\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone values should be filtered out.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_types:\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline parameter \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not found in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline job input definitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_version) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m ):\n\u001b[1;32m    231\u001b[0m     result \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: The pipeline parameter secret_file_path is not found in the pipeline job input definitions."
     ]
    }
   ],
   "source": [
    "run1 = aiplatform.PipelineJob( #->creamos una instancia de un job de Pipeline\n",
    "    display_name=\"custom-training-vertex-ai-pipeline\", #Nombre del pipeline\n",
    "    template_path=\"custom-pipeline-classifier.json\", #la ruta del archivo json que se generó utilizando el compilador, contiene la definición del pipeline compilado\n",
    "    job_id=\"custom-pipeline-7\", #identificador único para el job del pipeline\n",
    "    #parameter_values={\"bq_table\": \"fabian260497.dataset_prueba.iris\"},\n",
    "    parameter_values={\"bucket_name\": \"bucket-fabian-2\",\"file_path\":\"data_prueba/iris_dataset_2.csv\",\"bq_project\":\"fabian260497\",\"bq_dataset\":\"dataset_prueba\" ,\"bq_table\":\"iris\",\"secret_file_path\":\"etc/secrets/secret-sa.json\"},\n",
    "    enable_caching=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4f6880-ed8b-4e3b-918c-da24cc739a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/290399324087/locations/us-central1/pipelineJobs/custom-pipeline-7\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/290399324087/locations/us-central1/pipelineJobs/custom-pipeline-7')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-pipeline-7?project=290399324087\n"
     ]
    }
   ],
   "source": [
    "run1.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fe36ab7-21b6-4848-98f0-29b9f2bc378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import io\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# Reemplaza 'mi-bucket' con el nombre de tu bucket y 'archivo/datos.csv' con la ruta del archivo en el bucket\n",
    "bucket_name = \"bucket-fabian-2\"\n",
    "blob_name = 'data_prueba/iris_dataset_2.csv'\n",
    "\n",
    "#credenciales_json = 'ruta-a-tu-archivo-de-credenciales.json'\n",
    "#storage_client = storage.Client.from_service_account_json(credenciales_json)\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "# Lee los datos del Blob en un objeto bytes\n",
    "datos_bytes = blob.download_as_bytes()\n",
    "\n",
    "# Convierte los datos bytes en un DataFrame de pandas\n",
    "data_frame = pd.read_csv(io.BytesIO(datos_bytes))\n",
    "df= data_frame.head(50)\n",
    "df.to_csv(\"prueba.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2a4ef-d62f-4708-b7fe-d0d631568297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
